{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13695306,"sourceType":"datasetVersion","datasetId":8711204}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --force-reinstall numpy==1.26.4\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required survival analysis libraries\n!pip install pycox torchtuples lifelines","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install the missing codec package for LZW compression in TIFF files\n!pip install imagecodecs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Installation and Setup (FINAL)\n\nprint(\"Installing required Python packages...\")\n!pip install pycox torchtuples lifelines imagecodecs\n\n# --- Standard Libraries ---\nimport os\nimport re\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\n\n# --- PyTorch and Vision ---\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torchvision.models import ViT_B_16_Weights \nfrom torch.utils.data import Dataset, DataLoader\n\n# --- Survival Analysis and Evaluation (CRITICAL IMPORTS) ---\nfrom pycox.models import CoxTime\nfrom pycox.models import loss as pycox_loss # Import loss module for correct name\nimport torchtuples as tt\nfrom lifelines.utils import concordance_index\nfrom sklearn.model_selection import train_test_split\nfrom tifffile import imread # Assumes imagecodecs installed\n\n# --- CONSTANTS ---\nKAGGLE_INPUT_DIR = \"/kaggle/input/\" \nDATASET_NAME = \"time-series-dataset\" # ðŸš¨ UPDATE THIS TO YOUR SPECIFIC KAGGLE DATASET FOLDER NAME\nIMAGE_DIR = os.path.join(KAGGLE_INPUT_DIR, DATASET_NAME)\n\nFEATURE_OUTPUT_DIR = \"vit_extracted_features\"\n\n# Model Parameters\nVIT_OUT_DIM = 768    \nTRANSFORMER_DIM = 128\nMAX_SEQ_LEN = 5      \nIMG_SIZE = 224       \n\n# Training Parameters (Aggressive Regularization & Safer Learning)\nBATCH_SIZE = 16         # Reduced for stability and better CoxPH gradients\nNUM_EPOCHS = 200        # High epochs for full convergence\nLEARNING_RATE = 1e-5    # Very low/safer starting rate\nWEIGHT_DECAY = 1e-3     # Strong L2 Regularization (Penalizes large weights)\nGRADIENT_CLIP_VALUE = 1.0 \nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nprint(f\"\\nRunning on device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:10:08.918752Z","iopub.execute_input":"2025-11-18T17:10:08.919036Z","iopub.status.idle":"2025-11-18T17:10:16.346934Z","shell.execute_reply.started":"2025-11-18T17:10:08.919013Z","shell.execute_reply":"2025-11-18T17:10:16.346169Z"}},"outputs":[{"name":"stdout","text":"Installing required Python packages...\nRequirement already satisfied: pycox in /usr/local/lib/python3.11/dist-packages (0.3.0)\nRequirement already satisfied: torchtuples in /usr/local/lib/python3.11/dist-packages (0.2.2)\nRequirement already satisfied: lifelines in /usr/local/lib/python3.11/dist-packages (0.30.0)\nRequirement already satisfied: imagecodecs in /usr/local/lib/python3.11/dist-packages (2025.11.11)\nRequirement already satisfied: feather-format>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pycox) (0.4.1)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from pycox) (3.14.0)\nRequirement already satisfied: numba>=0.44 in /usr/local/lib/python3.11/dist-packages (from pycox) (0.60.0)\nRequirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from pycox) (1.2.2)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from pycox) (2.32.5)\nRequirement already satisfied: py7zr>=0.11.3 in /usr/local/lib/python3.11/dist-packages (from pycox) (1.0.0)\nRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from torchtuples) (1.26.4)\nRequirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from torchtuples) (2.2.3)\nRequirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from torchtuples) (3.7.2)\nRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.15.3)\nRequirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.8.0)\nRequirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.11/dist-packages (from lifelines) (0.5.0)\nRequirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.2.1)\nRequirement already satisfied: pyarrow>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from feather-format>=0.4.0->pycox) (19.0.1)\nRequirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\nRequirement already satisfied: narwhals>=1.17 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.48.1)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.15.0)\nRequirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->torchtuples) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->torchtuples) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->torchtuples) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->torchtuples) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->torchtuples) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->torchtuples) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->torchtuples) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->torchtuples) (2.9.0.post0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.44->pycox) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.4->torchtuples) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.4->torchtuples) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.4->torchtuples) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.4->torchtuples) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.4->torchtuples) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15.4->torchtuples) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->torchtuples) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->torchtuples) (2025.2)\nRequirement already satisfied: texttable in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (1.7.0)\nRequirement already satisfied: pycryptodomex>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (3.23.0)\nRequirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (1.1.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (7.1.3)\nRequirement already satisfied: pyzstd>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (0.18.0)\nRequirement already satisfied: pyppmd<1.3.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (1.2.0)\nRequirement already satisfied: pybcj<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (1.0.6)\nRequirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (0.2.3)\nRequirement already satisfied: inflate64<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from py7zr>=0.11.3->pycox) (1.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pycox) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pycox) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pycox) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->pycox) (2025.10.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.2->pycox) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.2->pycox) (3.6.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->torchtuples) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.15.4->torchtuples) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.15.4->torchtuples) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.15.4->torchtuples) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.15.4->torchtuples) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.15.4->torchtuples) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.15.4->torchtuples) (2024.2.0)\n\nRunning on device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Data Preprocessing and Metadata Generation (FINAL)\n\ndef create_image_metadata_df(image_path_root):\n    \"\"\"Parses filenames to extract Patient ID and a numerical time step.\"\"\"\n    file_paths = glob.glob(os.path.join(image_path_root, \"**\", \"*.tif\"), recursive=True)\n    records = []\n    # Adjust this regex pattern to match your file naming convention precisely!\n    pattern = re.compile(r'(\\w+)\\_SR\\_\\d+\\_IM(\\d+)\\.tif') \n    \n    for path in file_paths:\n        file_name = os.path.basename(path)\n        match = pattern.search(file_name)\n        if match:\n            patient_id = match.group(1).replace('_patient', '')\n            time_step = int(match.group(2)) \n            records.append({'patient_id': patient_id, 'time_step': time_step, 'file_path': path})\n            \n    df = pd.DataFrame(records).sort_values(['patient_id', 'time_step']).reset_index(drop=True)\n    return df\n\ndef generate_placeholder_survival_df(image_df):\n    \"\"\"\n    ðŸš¨ CRITICAL: REPLACE THIS WITH CODE TO LOAD YOUR ACTUAL SURVIVAL DATA.\n    (Placeholder data is kept for execution, but must be replaced for good results)\n    \"\"\"\n    unique_patients = image_df['patient_id'].unique()\n    np.random.seed(42)\n    # Generate less random placeholder data for better initial results\n    T = np.random.randint(100, 1500, len(unique_patients)).astype(np.float32)\n    E = np.random.randint(0, 2, len(unique_patients)).astype(np.int64) \n\n    survival_df = pd.DataFrame({'patient_id': unique_patients, 'time': T, 'event': E})\n    return survival_df\n\n# --- EXECUTION ---\nfull_metadata_df = create_image_metadata_df(IMAGE_DIR)\nsurvival_df = generate_placeholder_survival_df(full_metadata_df)\n\npatient_ids = full_metadata_df[['patient_id']].drop_duplicates()\nglobal final_df\nfinal_df = pd.merge(patient_ids, survival_df, on='patient_id') \n\nprint(f\"Total Unique Patients: {len(final_df)}\")\nprint(\"\\nSample Survival Data (time, event):\")\nprint(final_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:10:16.348244Z","iopub.execute_input":"2025-11-18T17:10:16.348657Z","iopub.status.idle":"2025-11-18T17:10:25.547349Z","shell.execute_reply.started":"2025-11-18T17:10:16.348634Z","shell.execute_reply":"2025-11-18T17:10:25.546684Z"}},"outputs":[{"name":"stdout","text":"Total Unique Patients: 455\n\nSample Survival Data (time, event):\n    patient_id    time  event\n0    137covid1  1226.0      1\n1   137covid10   960.0      0\n2  137covid100  1394.0      1\n3  137covid101  1230.0      1\n4  137covid103  1195.0      1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 4: Universal Survival Model Definition (ViT integrated into Transformer)\n\n# --- CRITICAL IMPORTS ---\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torchvision.models import ViT_B_16_Weights \nfrom torch.utils.data import Dataset, DataLoader\nfrom tifffile import imread\nfrom PIL import Image\nimport os\nimport numpy as np\n\n# --- 4.1. Positional Encoding Module ---\nclass SinusoidalPositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_SEQ_LEN):\n        super().__init__()\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# --- 4.2. Feature Dataset (Loads RAW Images) ---\nclass UniversalImageSequenceDataset(Dataset):\n    \"\"\"Loads raw image sequences and returns a padded tensor of images.\"\"\"\n    def __init__(self, dataframe, image_root_dir, max_seq_len, transform):\n        self.dataframe = dataframe\n        self.image_root_dir = image_root_dir\n        self.max_seq_len = max_seq_len\n        self.transform = transform\n        \n        # Pre-process metadata to link patient ID to a time-ordered list of paths\n        self.patient_sequences = self._get_patient_sequences()\n\n    def _get_patient_sequences(self):\n        # This re-runs the metadata parsing to get file paths (requires Cell 2 logic)\n        records = []\n        file_paths = glob.glob(os.path.join(self.image_root_dir, \"**\", \"*.tif\"), recursive=True)\n        pattern = re.compile(r'(\\w+)\\_SR\\_\\d+\\_IM(\\d+)\\.tif')\n        \n        for path in file_paths:\n            match = pattern.search(os.path.basename(path))\n            if match:\n                patient_id = match.group(1).replace('_patient', '')\n                time_step = int(match.group(2)) \n                records.append({'patient_id': patient_id, 'time_step': time_step, 'file_path': path})\n        \n        meta_df = pd.DataFrame(records).sort_values(['patient_id', 'time_step'])\n        \n        # Merge with survival data and create a sequence of paths for each patient\n        sequences = {}\n        for idx, row in self.dataframe.iterrows():\n            patient_id = row['patient_id']\n            patient_paths = meta_df[meta_df['patient_id'] == patient_id]['file_path'].tolist()\n            sequences[patient_id] = patient_paths\n        return sequences\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        patient_id = row['patient_id']\n        paths = self.patient_sequences.get(patient_id, [])\n        \n        image_tensors = []\n        for path in paths:\n            try:\n                # Load and prepare image (robustly handling TIFF)\n                img_array = imread(path)\n                if img_array.dtype != np.uint8:\n                    max_val = np.iinfo(img_array.dtype).max \n                    img_array = (img_array.astype(np.float32) / max_val * 255).astype(np.uint8)\n                if len(img_array.shape) == 2:\n                    img_array = np.stack([img_array]*3, axis=-1) \n                \n                img = Image.fromarray(img_array)\n                img_tensor = self.transform(img)\n                image_tensors.append(img_tensor)\n            except Exception:\n                # Use a zero tensor placeholder if load fails (important for sequence integrity)\n                image_tensors.append(torch.zeros(3, IMG_SIZE, IMG_SIZE))\n\n        # Pad/Truncate the sequence of tensors\n        seq_len = len(image_tensors)\n        \n        if seq_len > self.max_seq_len:\n            image_tensors = image_tensors[:self.max_seq_len]\n        elif seq_len < self.max_seq_len:\n            pad_tensor = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n            for _ in range(self.max_seq_len - seq_len):\n                image_tensors.append(pad_tensor)\n\n        X_sequence = torch.stack(image_tensors)\n        \n        time = row['time']\n        event = row['event']\n        \n        return X_sequence, (time, event)\n\n\n# --- 4.3. Universal Model (ViT + Transformer + CoxPH) ---\nclass UniversalSurvivalModel(nn.Module):\n    def __init__(self, input_dim=VIT_OUT_DIM, transformer_dim=TRANSFORMER_DIM):\n        super().__init__()\n        \n        # 1. Image Feature Extractor (ViT - NOW PART OF THE TRAINABLE NET)\n        self.vit = models.vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n        self.vit.heads = nn.Identity()\n        \n        # 2. Sequence Modeling Components\n        self.proj = nn.Linear(input_dim, transformer_dim)\n        self.pos_encoder = SinusoidalPositionalEncoding(d_model=transformer_dim)\n        self.dropout_in = nn.Dropout(p=0.25) \n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=transformer_dim, \n            nhead=4, \n            dim_feedforward=2 * transformer_dim, \n            batch_first=True, \n            dropout=0.25\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        # 3. Final Risk Output\n        self.cox_head = nn.Linear(transformer_dim, 1)\n\n    def forward(self, x_seq, t_star=None):\n        B, S, C, H, W = x_seq.shape # Batch, Sequence Length, Channel, Height, Width\n        \n        # Reshape for ViT: (B * S, C, H, W)\n        x_flat = x_seq.view(B * S, C, H, W)\n        \n        # 1. Feature Extraction (ViT)\n        with torch.no_grad(): # ViT is frozen for efficiency, fine-tune later if needed\n             features_flat = self.vit(x_flat) \n        \n        # Reshape back to sequence: (B, S, Feature Dim)\n        features = features_flat.view(B, S, -1)\n        \n        # 2. Sequence Processing (Transformer)\n        x = self.proj(features)                  \n        x = self.pos_encoder(x)          \n        x = self.dropout_in(x) \n        \n        z = self.transformer_encoder(x) \n        \n        final_z = z[:, -1, :] # Use the final sequence state\n        \n        h_hat = self.cox_head(final_z)\n        return h_hat.squeeze()\n        \n# --- ViT Preprocessing Transform (for DataLoader) ---\nimage_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:21:16.857250Z","iopub.execute_input":"2025-11-18T17:21:16.857992Z","iopub.status.idle":"2025-11-18T17:21:16.875258Z","shell.execute_reply.started":"2025-11-18T17:21:16.857961Z","shell.execute_reply":"2025-11-18T17:21:16.874600Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Cell 5: Training Function and Setup with EARLY STOPPING ðŸ›‘\n\n# --- CRITICAL IMPORTS (Ensure these are available from previous cells) ---\nfrom pycox.models import CoxTime\nfrom pycox.models.loss import CoxPHLoss \nimport torchtuples as tt\nfrom sklearn.model_selection import train_test_split\nimport numpy as np # Needed for infinity/best_loss tracking\n\n# FIX: Initialize the loss using the class name CoxPHLoss() directly\ncox_loss = CoxPHLoss()\n\n# --- EARLY STOPPING CONSTANT ---\nPATIENCE = 10 # Stop if validation loss doesn't improve for 10 epochs\nMAX_EPOCHS = 200 # Set a high ceiling just in case (the original 200)\n\n# --- EVALUATION FUNCTION (To get loss on validation/test data) ---\ndef evaluate_loss(model, data_loader, device):\n    \"\"\"Calculates the average CoxPH loss on the given data loader.\"\"\"\n    net = model.net\n    net.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for X_batch, (T_batch, E_batch) in data_loader:\n            X_batch = X_batch.to(device)\n            T_batch = T_batch.to(device)\n            E_batch = E_batch.to(device)\n            \n            h_hat = net(X_batch) \n            \n            loss = model.loss.forward(h_hat.cpu(), T_batch.cpu(), E_batch.cpu())\n            total_loss += loss.item() * len(X_batch) # Multiply by batch size to get total loss\n\n    return total_loss / len(data_loader.dataset) # Return average loss per sample\n\n# --- Training Function with EARLY STOPPING ---\ndef train_model_early_stop(model, train_loader, test_loader, device, max_epochs, patience):\n    \"\"\"Executes the training loop with Early Stopping based on test loss.\"\"\"\n    net = model.net \n    print(f\"\\n--- Starting Universal Model Training (Max {max_epochs} Epochs with Patience={patience}) ---\") \n    \n    best_loss = np.inf\n    patience_counter = 0\n    best_epoch = 0\n    \n    for epoch in range(1, max_epochs + 1):\n        # 1. Training Step\n        net.train()\n        total_train_loss = 0.0\n        for X_batch, (T_batch, E_batch) in train_loader:\n            # ... (Standard training loop remains the same) ...\n            X_batch = X_batch.to(device)\n            T_batch = T_batch.to(device)\n            E_batch = E_batch.to(device)\n            \n            model.optimizer.zero_grad()\n            h_hat = net(X_batch) \n            \n            loss = model.loss.forward(h_hat.cpu(), T_batch.cpu(), E_batch.cpu())\n            loss = loss.to(device)\n            \n            loss.backward()\n            nn.utils.clip_grad_norm_(net.parameters(), max_norm=GRADIENT_CLIP_VALUE)\n            model.optimizer.step()\n            total_train_loss += loss.item()\n            \n        avg_train_loss = total_train_loss / len(train_loader)\n\n        # 2. Validation Step (Early Stopping Check)\n        avg_val_loss = evaluate_loss(model, test_loader, device)\n        \n        print(f\"Epoch {epoch}/{max_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Patience: {patience_counter}\")\n        \n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            patience_counter = 0\n            best_epoch = epoch\n            # CRITICAL: Save the best model state dict\n            torch.save(net.state_dict(), 'best_model_state.pt') \n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"\\nðŸ›‘ EARLY STOPPING triggered at epoch {epoch}. Best loss: {best_loss:.4f} at epoch {best_epoch}.\")\n                break\n    \n    # Load the best weights back into the model before exiting\n    print(f\"Loading best weights from epoch {best_epoch}...\")\n    net.load_state_dict(torch.load('best_model_state.pt'))\n    \n    print(\"\\n--- Training Complete ---\") \n\n# --- EXECUTION SETUP ---\nprint(\"DEBUG: Starting Universal Model Setup and Raw Data Loading...\")\n\n# Data split\ndf_train, df_test = train_test_split(final_df, test_size=0.2, random_state=42)\n\n# Create Datasets and Loaders (Requires UniversalImageSequenceDataset and image_transform from Cell 4)\ntrain_dataset = UniversalImageSequenceDataset(df_train, IMAGE_DIR, MAX_SEQ_LEN, image_transform)\ntest_dataset = UniversalImageSequenceDataset(df_test, IMAGE_DIR, MAX_SEQ_LEN, image_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n# We need to use the test_loader for validation in the loop\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \nprint(f\"DEBUG: DataLoader initialized. Total training batches: {len(train_loader)}\")\n\n# Model Initialization (Requires UniversalSurvivalModel defined in Cell 4)\nnet = UniversalSurvivalModel().to(DEVICE)\n\n# Initialize Adam with HIGH WEIGHT_DECAY for L2 regularization\noptimizer = tt.optim.Adam(LEARNING_RATE, weight_decay=WEIGHT_DECAY) \nglobal model\nmodel = CoxTime(\n    net=net,\n    loss=cox_loss,\n    optimizer=optimizer, \n    device=DEVICE\n)\n\nprint(f\"DEBUG: CoxTime Model initialized. Device: {DEVICE}\")\n\n# Prepare targets for baseline hazard estimation later in Cell 6\nT_train_np = df_train['time'].values\nE_train_np = df_train['event'].values\nglobal tt_target_train\ntt_target_train = tt.tuplefy(T_train_np, E_train_np)\n\n# --- RUN TRAINING ---\ntry:\n    # Use the Early Stopping function\n    train_model_early_stop(model, train_loader, test_loader, DEVICE, MAX_EPOCHS, PATIENCE)\nexcept Exception as e:\n    print(f\"\\n!!! CRITICAL TRAINING ERROR: {e} !!!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:37:40.346582Z","iopub.execute_input":"2025-11-18T17:37:40.346897Z"}},"outputs":[{"name":"stdout","text":"DEBUG: Starting Universal Model Setup and Raw Data Loading...\nDEBUG: DataLoader initialized. Total training batches: 23\nDEBUG: CoxTime Model initialized. Device: cuda\n\n--- Starting Universal Model Training (Max 200 Epochs with Patience=10) ---\nEpoch 1/200 | Train Loss: 2.0379 | Val Loss: 1.8000 | Patience: 0\nEpoch 2/200 | Train Loss: 1.9711 | Val Loss: 1.7993 | Patience: 0\nEpoch 3/200 | Train Loss: 1.9669 | Val Loss: 1.7991 | Patience: 0\nEpoch 4/200 | Train Loss: 1.9655 | Val Loss: 1.7986 | Patience: 0\nEpoch 5/200 | Train Loss: 2.0113 | Val Loss: 1.7985 | Patience: 0\nEpoch 6/200 | Train Loss: 1.9631 | Val Loss: 1.7985 | Patience: 0\nEpoch 7/200 | Train Loss: 1.9739 | Val Loss: 1.7983 | Patience: 0\nEpoch 8/200 | Train Loss: 1.9659 | Val Loss: 1.7981 | Patience: 0\nEpoch 9/200 | Train Loss: 1.9440 | Val Loss: 1.7981 | Patience: 0\nEpoch 10/200 | Train Loss: 1.9320 | Val Loss: 1.7976 | Patience: 0\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Evaluation and Baseline Calculation (FINAL ROBUST SOLUTION - SHAPE FIX)\n\nimport torchtuples as tt \nimport numpy as np\nimport torch\nimport pandas as pd\nfrom lifelines.utils import concordance_index\n\n# --- BASELINE CALCULATION FUNCTION ---\ndef compute_baseline_and_ready_model(model, train_loader, tt_target_train, device):\n    \"\"\"Computes the baseline hazard, which enables predict_survival_function.\"\"\"\n    print(\"\\nDEBUG: Estimating Baseline Survival Function...\")\n    \n    X_train_list = [X_batch for X_batch, _ in train_loader]\n    X_train_all_unsorted = torch.cat(X_train_list) \n    \n    T_train_np = tt_target_train[0]\n    E_train_np = tt_target_train[1]\n    sort_idx = np.argsort(T_train_np)\n\n    X_train_all_sorted = X_train_all_unsorted[sort_idx].to(device)\n    T_train_np_sorted = T_train_np[sort_idx]\n    E_train_np_sorted = E_train_np[sort_idx]\n\n    model.compute_baseline_hazards(\n        input=X_train_all_sorted, \n        target=tt.tuplefy(T_train_np_sorted, E_train_np_sorted)\n    )\n    print(\"DEBUG: Baseline Survival Function estimated successfully.\")\n    return model\n\n# --- EVALUATION FUNCTION ---\ndef evaluate_model(model, test_loader, device):\n    \"\"\"Calculates the Concordance Index (C-index) and Survival Functions.\"\"\"\n    net = model.net\n    net.eval()\n    \n    h_hat_list, T_list, E_list, X_list = [], [], [], []\n    \n    with torch.no_grad():\n        for X_batch, (T_batch, E_batch) in test_loader:\n            X_batch = X_batch.to(device)\n            h_hat = net(X_batch)\n            h_hat_list.append(h_hat.cpu().numpy().flatten())\n            T_list.extend(T_batch.cpu().numpy())\n            E_list.extend(E_batch.cpu().numpy())\n            X_list.append(X_batch.cpu())\n\n    h_test_all = np.concatenate(h_hat_list)\n    T_test_all = np.array(T_list)\n    E_test_all = np.array(E_list)\n    \n    c_index = concordance_index(T_test_all, -h_test_all, E_test_all) \n    \n    if hasattr(model, 'predict_survival_function'):\n        surv_method = model.predict_survival_function\n    elif hasattr(model, 'predict_surv'):\n        surv_method = model.predict_surv\n    else:\n        raise AttributeError(\"Prediction method not found.\")\n\n    surv_output = surv_method(tt.tuplefy(torch.cat(X_list)))\n\n    return c_index, surv_output\n\n# --- EXECUTION ---\n\n# 1. Prepare the model by calculating the baseline hazard\nglobal model\nmodel = compute_baseline_and_ready_model(model, train_loader, tt_target_train, DEVICE)\n\n# 2. Run the final evaluation\nc_index, surv_result = evaluate_model(model, test_loader, DEVICE) \n\n# ðŸš¨ CRITICAL FIX: Transpose the array (surv_result.T) to match the index shape\nif isinstance(surv_result, np.ndarray) or isinstance(surv_result, torch.Tensor):\n    # Transpose the prediction array before creating the DataFrame\n    global surv_df\n    surv_df = pd.DataFrame(surv_result.T, index=model.baseline_hazards_.index) \nelif hasattr(surv_result, 'to_pandas'):\n    surv_df = surv_result.to_pandas()\nelse:\n    surv_df = surv_result\n\nprint(f\"\\n--- Model Performance ---\")\nprint(f\"Test Concordance Index (C-index): {c_index:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:37:36.333062Z","iopub.status.idle":"2025-11-18T17:37:36.333296Z","shell.execute_reply.started":"2025-11-18T17:37:36.333187Z","shell.execute_reply":"2025-11-18T17:37:36.333199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Final Visualization and CLINICAL RESOURCE PREDICTION ðŸš‘\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- 7.1. Clinical Prediction Constants ---\nCRITICAL_TIME = 730 # 2 years (730 days) - The timeframe for the prediction\nMAX_FINANCIAL_LOSS = 50000.0 # Kept for the Expected Loss calculation\n\n# --- NEW CLINICAL THRESHOLDS ---\n# Probability of Survival (S(t)) thresholds for clinical placement at T_critical\nP_ICU_THRESHOLD = 0.40      # Survival <= 40% requires ICU\nP_WARD_THRESHOLD = 0.70     # Survival <= 70% requires Inpatient Ward\n\n# --- 7.2. Core Prediction Functions ---\n\ndef calculate_mean_time_to_event(surv_df):\n    \"\"\"Calculates Mean Time to Event (MTTE).\"\"\"\n    return surv_df.apply(lambda col: np.trapz(col.values, surv_df.index.values), axis=0)\n\ndef get_survival_at_critical_time(surv_df, critical_time):\n    \"\"\"Safely retrieves predicted S(t) at the critical time using interpolation.\"\"\"\n    s_t = surv_df.apply(lambda col: np.interp(critical_time, surv_df.index.values, col.values), axis=0)\n    return s_t\n\ndef assign_clinical_triage(s_t, p_icu, p_ward):\n    \"\"\"\n    Assigns each patient to a level of care based on their predicted Survival Probability.\n    \"\"\"\n    def assign_care_level(prob_survival):\n        if prob_survival <= p_icu:\n            return '1 - Critical: ICU Admission'\n        elif prob_survival <= p_ward:\n            return '2 - Serious: Inpatient Ward'\n        else:\n            return '3 - Stable: Outpatient Follow-up'\n\n    return s_t.apply(assign_care_level).rename('Clinical_Care_Level')\n\ndef calculate_risk_volatility(surv_df, critical_time):\n    \"\"\"Calculates the Volatility Score (rate of risk change).\"\"\"\n    gradients = surv_df.apply(lambda col: np.gradient(col.values, surv_df.index.values), axis=0)\n    \n    def interpolate_gradient(gradients_series, index, critical_time):\n        return np.interp(critical_time, index.values, gradients_series.values)\n\n    volatility_at_t = gradients.apply(lambda col: interpolate_gradient(col, surv_df.index, critical_time), axis=0)\n    abs_volatility = np.abs(volatility_at_t)\n    min_vol, max_vol = abs_volatility.min(), abs_volatility.max()\n    \n    if max_vol == min_vol:\n        volatility_score = pd.Series(50, index=abs_volatility.index)\n    else:\n        volatility_score = 100 * (abs_volatility - min_vol) / (max_vol - min_vol)\n        \n    return volatility_score.rename('Volatility_Score')\n\n\n# --- 7.3. EXECUTION AND VISUALIZATION ---\nprint(\"\\n--- CLINICAL RESOURCE PREDICTION & FINANCIAL RISK ANALYSIS ---\")\n\n# 1. Get Survival Probability at T_critical\nsurvival_at_t = get_survival_at_critical_time(surv_df, CRITICAL_TIME)\n\n# 2. NEW: Assign Clinical Care Level (Predicting ICU Need)\npatient_clinical_triage = assign_clinical_triage(survival_at_t, P_ICU_THRESHOLD, P_WARD_THRESHOLD)\n\n# 3. Calculate Volatility Score\npatient_volatility = calculate_risk_volatility(surv_df, CRITICAL_TIME)\n\n# 4. Calculate Expected Loss (Financial Metric)\nprob_event = 1.0 - survival_at_t\nexpected_loss = prob_event * MAX_FINANCIAL_LOSS\n\n# --- PRINTING RESULTS ---\nprint(f\"Prediction Window: T={CRITICAL_TIME} Days.\")\n\nprint(f\"\\n1. Patient Clinical Care Level (ICU Prediction): ðŸ©º\")\nprint(patient_clinical_triage.head())\n\nprint(f\"\\n2. Patient Expected Loss (Financial Risk): ðŸ’µ\")\nprint(expected_loss.head().map('${:,.2f}'.format))\n\nprint(f\"\\n3. Patient Survival Volatility Score (Rate of Risk Change): ðŸ’¥\")\nprint(patient_volatility.head().map('{:.1f}'.format))\n\n\n# --- Descriptive Statistics of Clinical Triage ---\nclinical_counts = patient_clinical_triage.value_counts(normalize=True).map('{:.1%}'.format)\nprint(f\"\\n--- Hospital Resource Allocation Summary ---\")\nprint(clinical_counts)\n\n\n# --- 7.4. Visualization ---\nplt.figure(figsize=(12, 7))\n\ntriage_groups = patient_clinical_triage.unique()\npalette = {\n    '1 - Critical: ICU Admission': 'red', \n    '2 - Serious: Inpatient Ward': 'orange', \n    '3 - Stable: Outpatient Follow-up': 'green'\n}\n\nfor group in triage_groups:\n    patients_in_group = patient_clinical_triage[patient_clinical_triage == group].index\n    \n    if len(patients_in_group) > 0:\n        surv_df[patients_in_group].iloc[:, :min(5, len(patients_in_group))].plot(\n            legend=False, \n            alpha=0.6, \n            color=palette.get(group, 'blue'),\n            ax=plt.gca(),\n            label=group if group == triage_groups[0] else \"\" \n        )\n\nplt.title('Survival Curves Grouped by Predicted Clinical Care Level')\nplt.ylabel('Probability of Survival, S(t)')\nplt.xlabel(f'Time (Duration in days)')\n\nplt.axvline(CRITICAL_TIME, color='r', linestyle=':', label=f'Critical Prediction Time ({CRITICAL_TIME} days)')\n\n# Create custom legend for the groups\nfrom matplotlib.lines import Line2D\nlegend_elements = [\n    Line2D([0], [0], color=palette['1 - Critical: ICU Admission'], lw=3, label=f'ICU (S(t) $\\\\leq {P_ICU_THRESHOLD:.0f}$%)'),\n    Line2D([0], [0], color=palette['2 - Serious: Inpatient Ward'], lw=3, label=f'Ward (S(t) $\\\\leq {P_WARD_THRESHOLD:.0f}$%)'),\n    Line2D([0], [0], color=palette['3 - Stable: Outpatient Follow-up'], lw=3, label='Outpatient'),\n    Line2D([0], [0], color='r', linestyle=':', lw=1, label=f'T={CRITICAL_TIME} Days')\n]\nplt.legend(handles=legend_elements, loc='best')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:37:36.334517Z","iopub.status.idle":"2025-11-18T17:37:36.334793Z","shell.execute_reply.started":"2025-11-18T17:37:36.334691Z","shell.execute_reply":"2025-11-18T17:37:36.334702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}